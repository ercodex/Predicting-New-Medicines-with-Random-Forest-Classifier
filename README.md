# *If you’re looking for a story of triumph, you’re in the wrong place. If you want to see how grit and curiosity can turn failures into lessons, welcome aboard!*

# "Shittiest Model of the Competition" Award Winner!
- This model was my submission for the Kaggle competition **"NeurIPS 2024 - Predict New Medicines with BELKA"**.

## Story
- Fresh off completing a machine learning course on Udemy, I thought, "Why not test my skills?" So I jumped into this competition... solo. Spoiler alert: the DL veterans crushed me.  
- Here's what I had: 8 GB of DDR4 RAM, a trusty GTX 1650 GPU, minimal ML experience, zero knowledge of neural networks, and, of course, my unmatched stubbornness.

## What I Did
1. **Understanding the Data**  
   - The dataset was biological and massive: 3 protein targets, millions of molecule sequences generated by ~300 molecules. The task? Predict which molecules are likely to bind to these targets. Cool concept, overwhelming data.  

2. **The 50 GB Challenge**  
   - Yep, 50 GB of data. My RAM said, "Nope." I tried compressing, transforming, mapping... eventually shrunk it to 35 GB. Still not enough.  
   - A college instructor gave me a golden tip: "If they can zip the download file to 5 GB for competitors, you can figure something out." Inspired, I spent **2 months** creating mapping tables, using the less-space-consuming data types , and trimming down data to a manageable 15 GB. Turns out, others in the discussion forum were struggling too.

3. **Building the Model**  
   - With the clock ticking, I went for the two most practical option for me: **Random Forest Classifier**, or **Stochastic Gradient Descent Classifier**. No fancy hyperparameter tuning, there was barely any time.
   - Training posed another hustle. The dataset was too large for my setup, and NumPy just didn’t cut it. Enter **Dask library**, the lifesaver! It handled chunked training like a charm, and I managed to train, predict, and submit just before the deadline.
   - Each training session nearly took 8 hours. My laptop was working like a full-time employee.

4. **The Results**  
   - Final ranking: **1925/1952**. Sure, I wasn’t at the top, but the journey taught me invaluable lessons in data handling, persistence, and problem-solving. Plus, it was hilarious!

5. **Special Thanks**
   - A shoutout to my instructor who guide the way to the model. **Not turning away an eager student can change many things.**  
   - A shoutout to **ChatGPT**, which sped up my problem-solving process. While I didn’t use it to code the model, it served as a trusted guide whenever I got stuck.

## What I Learned
- Learned techniques to handle **large datasets**.  
- Discovered the **Dask** library.
- Learned about **SGD Classifier**
- Gained hands-on experience with .csv and .parquet formats.
- SMILES format for representing biological data.
- Gained practical experience in **data visualization** and **feature engineering**.  
- Realized I need to start training myself in **Deep Learning**.  
- Built connections with instructors by showcasing my efforts.  
- Most importantly, I had fun and grew as a Computer Engineering student.
